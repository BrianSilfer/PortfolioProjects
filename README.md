# PortfolioProjects

For this portfolio project, I wanted to showcase my hard skills in data analysis to analyze a dataset of 500 auto insurance customers who had been in an accident. This dataset was found on kaggle.com. I wanted to show my ability to use SQL for data exploration and table generation. The queries that I completed in SQL were then used to create a customer profile dashboard using Tableau. I wanted to make this portfolio project a three-step process of data exploration/profiling, data visualization, and data analysis. The data analysis step was done using Python pandas. 

	The first stage of this project includes splitting the original data set into two parts, then combining that dataset using a join statement. I then used SQL to create summary tables to answer specific questions I had regarding the data. Those questions included, how many customers were there in each state, what are the average ages, and what is the average policy duration for customers insured in each state. I also wanted to know more about the distribution of accident claims across gender, as well as the distribution of accidents across the different education levels of the insured. The results of these queries would be exported in CSV format for use in Tableau. The dashboard I created in Tableau shows a sample of visualizations showing different attributes and summary statistics of the customer data. I wanted to create this dashboard to give a glance at some information regarding the customers within the dataset to start to formulate some hypotheses, to be tested later in the analysis stage. 

Those hypotheses were that the education level of the insured, gender, and occupation would have an impact on the total claim amount. My first step after loading the joined dataset into pandas, was to finish the data cleaning process. I would have liked to have done all of the data cleaning in SQL, but DML commands weren't allowed in the Big Query Sandbox. The leftover cleaning that needed to be done included dropping columns, inspecting the column data types, and filtering each column to make sure that no values needed to be combined or replaced. The only cleaning that had to be done was I wanted to replace “?” in the collision type and property damage columns with “Unknown”. This made more sense to me and makes the data easier to understand. I then began my analysis of the newly cleaned dataset. 
	
For starters, I wanted to provide an example of how to use the .groupby function to create a summary table of average values. I chose to group by the make of customer cars, which creates a table of average values based on the make of customer cars in the dataset. From there I showed how to use the .describe method to provide a quick summary statistics table of total claim amounts across the entire dataset. I also included the variance, median, and standard error. These statistics further describe the dataset and are useful during a general analysis of the data. After providing summary statistics for the total claim amount, I wanted to create some visualizations using Pyplot. I wanted to look at the distributions of total claim amount across gender, education level, and occupation of the insured as well as the auto make of the insured's car. This gave me the ability to answer some of my initial questions about the data. I was able to conclude that the distribution of total claim amounts across gender and education level is fairly even. I was surprised by the distribution of total claim amounts versus occupation, as managerial/executive roles had more than double the total claim amounts as insureds who worked in admin-clerical roles. Based on my experience as a claims adjuster, expensive vehicles always cost more to repair, which could indicate the occupation of an insured increases the likelihood that a claim will result in a higher payout because people working in higher-paying positions could have more expensive vehicles. More expensive vehicles almost always cost more to repair due to the cost of extremely high quality parts as well as the exclusiveness of the parts themsevles. Most vehicle owners like to have new parts put on their car as opposed to after-market parts, but this was most often the case with expensive vehciles. This creates an interesting dynamic as the occupation criteria for insureds could be looked at in a new light with this information. Occupation as a metric for driver safety could be misleading as an insurance company trying to keep the total claim amounts as low as possible in the future.

	After completing that deeper analysis, I wanted to create some scatter plots of different variables of interest as a precursor to my correlation analysis. These variables included insured age, property claim amount, and injury claim versus total claim amount. Age had very little correlation to the total claim amount while property and injury claims had a high degree of correlation to the total claim amount, which makes perfect sense. As property claim and injury claim amounts increase, so does the total claim amount. This led me to my correlation analysis using Seaborn’s heatmap feature and the .corr() method to create a correlation table of numerical values within the claims dataset. After reviewing the correlation table and heatmap, there were very few surprising correlations. I wanted to utilize all of the data, even the categorical variables in the dataset using the .corr() method, so I used Python to change the values within the columns that were of the data type: object, assigning them a numerical value. This allows them to be utilized with the .corr() method. Due to the large size of variables now incorporated into the correlation table, I wanted to filter the results down to only show correlation pairs with correlation coefficients over 0.5. After completing this filtering, there were still not very many surprises. Claims amount data and age/months_as_customer data were highly correlated, which makes sense. I was surprised by this as I expected age to have a high correlation with the total claim amount. 

	This portfolio project displays a sample of how I would explore, clean, analyze, and visualize a dataset using SQL, Python pandas, and Tableau. I will also provide work samples showing more of my coding skills in Python, as I have several other coding skills I didn’t necessarily utilize for this project.

